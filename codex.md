Here‚Äôs a single **mega prompt** you can paste into Codex / Cursor / Claude Code / whatever you‚Äôre using as your coding agent.

I‚Äôve woven together:

* your **existing repo structure** (longevity_conversation.py, longevity_agents.py, Valyu MCP, etc.)
* the **A2A longevity design** (two agents negotiating a 6-month plan)
* **Valyu search / scientific validation**
* **reliability + tests + LangSmith/telemetry**
* **ElevenLabs-style UI / workflow builder metaphor**
* **hackathon Track A ‚ÄúAgent Iron Man‚Äù win condition**

You can treat this as a *system prompt* for the coding assistant.

---

## üî• MEGA PROMPT FOR CODING AGENT

You are an expert AI engineer and software architect helping a small team at a hackathon (Holistic AI x UCL ‚Äì Track A ‚ÄúAgent Iron Man‚Äù) improve an existing **multi-agent longevity planning system**.

Your job is to **extend and harden the existing Python repo**, not to start from scratch.

The goal is to demonstrate **reliability, robustness, scientific rigor, and observability** in a **10-turn agent-to-agent conversation** that produces a **personalized, evidence-based 6-month longevity plan**.

---

## 0. Repo Overview (Current State)

The repo already contains:

* **Core entry point**

  * `longevity_conversation.py` ‚Äì orchestrates a multi-turn conversation between two OpenAI Assistants. (Starting with 10)
* **Agents**

  * `longevity_agents.py` ‚Äì defines two agents:

    * **Customer agent** ‚Äì ‚ÄúCustomer Health Advocate‚Äù
    * **Company agent** ‚Äì ‚ÄúCompany Service Advisor‚Äù
* **Persistence / logging**

  * `save_conversation.py` ‚Äì helper to save conversations to timestamped text files.
  * Each run writes to `data/longevity_plan_<timestamp>/`:

    * `conversation_history.txt` ‚Äì all turns, timestamped.
    * `longevity_plan_summary.txt` ‚Äì final structured summary (generated by summarizer assistant).
    * `scientific_validity_checks.json` ‚Äì Valyu results (when available).
* **Inputs / data sources**

  * `user_info.json` ‚Äì detailed user profile: health, lifestyle, goals, history, preferences, availability, biomarkers.
  * `company_resource.txt` ‚Äì company services, prices, eligibility, contraindications, scheduling rules, policies.
* **Environment & telemetry**

  * `.env` ‚Äì API keys (expects at least `OPENAI_API_KEY_1`; optional LangSmith keys, Holistic API, Bedrock keys).
  * `scripts/setup_env.py` ‚Äì sets up environment, connects to Holistic AI Bedrock/OpenAI, and optionally LangSmith.
  * `scripts/telemetry-test.py` ‚Äì example LangGraph ReAct agent + LangSmith tracing demonstration.
* **Valyu scientific validation**

  * Integrated via **MCP-style HTTP call**:

    * Heuristic extraction of sentences that look like scientific claims (keywords like ‚Äústudies show‚Äù, ‚Äúclinical trial‚Äù, ‚Äúproven‚Äù, ‚Äúimproves‚Äù, etc.).
    * Optional POST to `http://localhost:3000/validate` with `{claim, context}`.
    * Aggregates results to `scientific_validity_checks.json`.
    * Handles server unavailability gracefully.
* **Summarization**

  * Uploads `conversation_history.txt` to OpenAI Files.
  * Creates a temporary ‚ÄúLongevity Plan Summarizer‚Äù assistant.
  * Produces a structured ‚ÄúLONGEVITY PLAN SUMMARY‚Äù written to `longevity_plan_summary.txt`.
* **Tutorials & resources**

  * `tutorials/*.ipynb` for LangGraph, tools, structured output, LangSmith, evaluation, RL, red-teaming.
  * `tutorials/holistic_ai_bedrock.py` for Bedrock ‚Üî LangChain wrapper.
  * `resources/TRACK_*.md` with hackathon guidance.

Known **limitations / gaps** (these are what you must address):

* No real scheduling backend (appointments are just text, no calendar/slots).
* Valyu integration is basic (one endpoint; no advanced search or multi-claim workflows).
* No CLI flags or config file (turns, model, Valyu URL, etc. are hardcoded).
* Uses `code_interpreter` tool but no real tool orchestration.
* Reliability tests are not implemented (no chaos tests, consistency tests, etc.).
* No UI, but we want to mirror **ElevenLabs Agent** style (decision tree, analysis, tools, tests tabs).

---

## 1. Target System Design (High-Level)

You must **evolve** this repo into a reliability-focused A2A system with the following properties:

### Purpose

> ‚ÄúA multi-agent system that, over **10 turns**, negotiates a **personalized, evidence-based 6-month longevity plan**, validates scientific claims via **Valyu**, and outputs a structured, testable result.‚Äù

### Agents (rename logically, keep compatibility)

We keep the two roles conceptually, but structure them more cleanly:

1. **User-Side Agent ‚Äì ‚ÄúHealth Advocate‚Äù**
   ‚Äì Represents the patient; pulls from `user_info.json`.
   ‚Äì Ensures user goals, constraints, budget, preferences, schedule, and safety are respected.
   ‚Äì Never flips into ‚Äúcompany‚Äù role.

2. **Clinic-Side Agent ‚Äì ‚ÄúService Planner‚Äù**
   ‚Äì Represents the longevity clinic / company; uses `company_resource.txt`.
   ‚Äì Proposes services, bundles, timelines, and costs based on eligibility rules and policies.
   ‚Äì Ensures no service violates contraindications or company policies.
   ‚Äì Never pretends to be the user.

Both agents:

* **Are explicitly non-diagnostic** and must not give medical diagnoses or prescriptions.
* May suggest **lifestyle/educational** steps and **referrals** to human clinicians.

### Conversation Orchestration

* Continue to use **Assistants API** threads & runs.
* Keep the **turn limit** (default 10) but make it configurable.
* Flow:

  1. Seed a first message from **Health Advocate** based on `user_info.json` (‚ÄúI represent Alice; here are their goals‚Ä¶‚Äù).
  2. Alternate:

     * Planner responds
     * Advocate responds
  3. After each turn:

     * Append to transcript
     * Extract potential scientific claims
  4. At the end:

     * Run Valyu validations for collected claims
     * Run summarizer assistant to produce **6-month plan**

---

## 2. Scientific Claim Validation (Valyu MCP ‚Äì EXTEND & HARDEN)

We already have keyword-based claim extraction and POST to `http://localhost:3000/validate`. You must:

1. **Refine claim extraction**:

   * Use slightly smarter heuristics:

     * Look for sentences with:

       * action + outcome (‚ÄúX reduces Y‚Äù, ‚ÄúX improves Y‚Äù)
       * mentions of ‚Äúrisk‚Äù, ‚Äúmortality‚Äù, ‚Äúlongevity‚Äù, ‚Äúbiomarker‚Äù
     * Limit to sentences above a length threshold (e.g., > 40 chars).
   * Tag each claim with:

     * `turn_index`
     * `speaker` (Advocate / Planner)
     * `context` (preceding and following sentences).

2. **Batch Valyu queries** where possible:

   * Support sending multiple claims in one request if the Valyu API allows (you can mock this if needed).
   * If not, sequential calls with robust retry and timeout.

3. **Valyu search semantics**:

   * Treat `http://localhost:3000/validate` as a **search/validation engine** that returns:

     * `validity` (true/false/unknown)
     * `confidence` (0‚Äì1)
     * `evidence` (citations, or summary)
   * Store **full response** per claim in `scientific_validity_checks.json`.

4. **Propagation back to plan**:

   * If a recommended service or claim is flagged as invalid / low-confidence:

     * Add a warning section in the final summary.
     * Optionally tag plan items as:

       * ‚ÄúEvidence-uncertain ‚Äì review with clinician.‚Äù

5. **Robustness**:

   * If Valyu is unavailable:

     * Mark `server_unavailable: true` and set `validity: "unknown"`.
     * Do *not* crash the main conversation.
     * Print a clear log message in stdout and in summary.

---

## 3. Scheduling Backend (Mocked but Realistic)

Currently, ‚Äúappointments‚Äù are just text.

Create a **mock scheduling layer**:

* `scheduling/clinic_scheduler.py`:

  * Define a simple set of **slots**:

    * dates & times over the next 6 months
    * staff types (doctor, nurse, lab tech)
    * services (e.g., baseline bloodwork, VO‚ÇÇ test, scan, lifestyle coaching)
  * Provide functions:

    * `find_available_slots(service_type, user_availability, blackout_dates)`
    * `book_slot(service_type, user_id)` that:

      * reserves a slot in an in-memory or JSON store.
      * returns a structured appointment object.

Integrate this by:

* Having the **Service Planner agent** propose services with **desired windows**.
* A scheduling tool (callable from Python, not necessarily from the LLM) chooses concrete slots.
* The **plan summary** includes:

  * exact dates/times
  * staff role
  * location (mocked)
  * cost per appointment
  * monthly breakdown.

If you want to keep it simple, you can:

* Generate deterministic sample slots.
* Ensure that multiple runs with the same input produce **consistent bookings** unless we intentionally inject randomness.

---

## 4. Configurability & CLI

Add a CLI interface in `longevity_conversation.py` (or a new `main.py`) with:

* Flags / config for:

  * `--turn-limit` (default 10)
  * `--model` (default `gpt-4o-mini`)
  * `--valyu-url` (default `http://localhost:3000/validate`)
  * `--enable-valyu` (bool)
  * `--user-profile` path (default `user_info.json`)
  * `--company-resource` path (default `company_resource.txt`)
  * `--output-dir` (default `data/`)

Use `argparse` or `typer`.

This allows us to run multiple simulation batches and stress tests from the command line.

---

## 5. Reliability & Evaluation (Track A ‚ÄúAgent Iron Man‚Äù Requirements)

We must show **reliability**, not just capabilities.

Implement the following in a `tests/` directory (pytest encouraged):

### 5.1. Consistency Tests ‚Äì Plan & Scheduling Consistency

* Given the same `user_info.json` and `company_resource.txt`:

  * Run the 10-turn conversation **N times** (e.g., 10).
  * Extract:

    * plan structure (services, timings)
    * final total cost
  * Define a metric:

    * e.g., **‚â• 90‚Äì95%** of runs produce the *same* or *semantically equivalent* plan (same services & schedule, minor textual differences allowed).
* Save stats to a JSON report (e.g. `tests/results/consistency_report.json`).

### 5.2. Chaos / Fault Injection Tests

Create a small **fault injection layer** around:

* Valyu HTTP calls
* Scheduling backend
* LLM calls (simulate intermittent exceptions / timeouts)

In `tests/test_chaos.py`:

* Randomly:

  * drop Valyu responses
  * delay responses (simulate latency)
  * cause scheduler to reject a booking
* Ensure:

  * The system doesn‚Äôt crash.
  * It logs clear errors.
  * It informs the user in the summary when something couldn‚Äôt be scheduled or validated.
  * It follows fallback behaviors, e.g.:

    * ‚ÄúIf scheduler fails, propose ‚Äòcall clinic to book‚Äô instead of a concrete time.‚Äù

### 5.3. Load Tests ‚Äì Parallel Conversations

In `tests/test_load.py` (and optionally `scripts/run_load_test.py`):

* Use `asyncio` or threading to run **multiple conversations in parallel** (e.g., 10‚Äì20).
* Measure:

  * mean & p95 latency per conversation
  * error rate
* Verify:

  * no race conditions on file outputs
  * per-run directories are unique
  * no cross-run contamination.

### 5.4. Role Confusion Tests

Given your prompts already say ‚ÄúNever flip roles,‚Äù you must **test** this.

In `tests/test_role_confusion.py`:

* Analyse conversation logs to ensure:

  * Health Advocate never refers to itself as the clinic.
  * Service Planner never speaks as the user.
  * There is no confusing ‚ÄúI will come to your appointment at 3 PM‚Äù coming from the wrong agent.
* If there is evidence of role confusion, tweak prompts and re-run until tests pass.

### 5.5. Scientific Safety Tests

In `tests/test_scientific_safety.py`:

* Inject synthetic ‚Äúbad‚Äù services into `company_resource.txt` for a special test profile (e.g., obviously harmful or obviously unsupported).
* Confirm:

  * Either they never get recommended, or
  * When recommended, Valyu marks them as low-validity and the final plan includes a strong warning / exclusion.

---

## 6. Observability and Telemetry (LangSmith / Logging)

Leverage `scripts/setup_env.py` and `scripts/telemetry-test.py` to:

* Wrap LLM calls and tool calls with **LangSmith tracing** using environment variables:

  * `LANGSMITH_API_KEY`
  * `LANGSMITH_TRACING=1`
* For each conversation run, record:

  * tokens per step
  * latency per call
  * tool calls (Valyu, scheduler)
  * errors
* Save a local JSON log per run (`telemetry.json`) in the output directory.

We want to be able to show the judges:

* a LangSmith link (if available) for at least one run
* or failing that, clean structured logs we can visualize.

---

## 7. ElevenLabs-Style UI / Workflow Metaphor (Lightweight)

We don‚Äôt need a full production UI, but we need a **basic representation** that mimics ElevenLabs Agent Studio tabs:

* **Agent tab** ‚Äì show:

  * Health Advocate & Service Planner prompts
  * persona config (tone, language, goals)
* **Workflow tab** ‚Äì show:

  * states:

    * Start ‚Üí Intake ‚Üí PlanDraft ‚Üí PlanReview ‚Üí Audit ‚Üí Revision ‚Üí FinalPlan ‚Üí Scheduling ‚Üí FinalSummary
* **Tools tab** ‚Äì show:

  * Valyu validator
  * Scheduler
  * File saving & summarizer
* **Analysis tab** ‚Äì show:

  * recent runs (just list conversation directories)
  * load & consistency reports
* **Tests tab** ‚Äì show:

  * list of pytest test groups and pass/fail status.

A minimal approach:

* Implement a **simple CLI or text UI** in `scripts/overview.py` that:

  * prints the workflow graph (ASCII)
  * lists tools & tests
  * shows recent run stats.

If time allows, you can implement a small **Streamlit** or **simple Flask + HTML** dashboard, but it‚Äôs optional. Reliability/test tooling is higher priority.

---

## 8. Safety & Compliance Framing (Important)

Throughout all prompts and logic:

* Do **not** implement diagnosis or ‚Äútreatment protocols‚Äù.
* Ensure all outputs are framed as:

  * educational,
  * lifestyle guidance,
  * suggestions to discuss with a licensed clinician.
* When describing therapies, always include:

  * risk disclaimers,
  * suggestion to confirm with a doctor.

If necessary, add a **post-processor** to the final summary to make sure disclaimers are always included.

---

## 9. Coding Style & Implementation Expectations

When you generate or modify code:

* Keep existing files when possible; extend them rather than rewriting everything.
* Add docstrings and type hints where reasonable.
* Keep file paths consistent (`data/`, `scripts/`, `tests/`).
* Do not hard-code secret keys.
* Make sure the repo can be set up with:

  * `pip install -r requirements.txt` or similar.
  * `python longevity_conversation.py` for a basic run.
  * `pytest` for tests.

Your output should be **specific code changes** and new files, not just prose.

---

## 10. Execution Order (What You Should Do First)

1. **Read and summarize** the current code in:

   * `longevity_conversation.py`
   * `longevity_agents.py`
   * `save_conversation.py`
   * `scripts/setup_env.py`
2. **Refactor / extend** conversation orchestration to support:

   * config/CLI
   * scheduling backend integration
   * improved Valyu pipeline
3. **Add schemas / helper classes** (if needed) for:

   * claims
   * appointments
   * plan summary structure
4. **Implement tests** under `tests/` as described.
5. **Implement telemetry logging**.
6. **Add the lightweight workflow/overview UI script**.
7. **Run tests & one demo run**, adjust prompts until:

   * role confusion minimized
   * claim validation recorded
   * summary structured nicely.

At every step, prefer **working, test-backed code** over speculative architectures.

---

Use this entire specification as your **global context** when editing the repo.
